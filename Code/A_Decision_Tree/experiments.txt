Mutual Information(Base Data):
	I(Random,Random) = 0.003

	I(W_k_c,Class) = 0.1653
	I(W_k_r,Class) = 0.2903
	I(W_r_c,Class) = 0.0501
	I(W_r_r,Class) = 0.0462
	I(B_k_c,Class) = 0.1851
	I(B_k_r,Class) = 0.3145

	I(Edge-Distance,Class) = 0.2602
	I(Kings-Distance,Class):
		L1= 0.1068
		L2 = 0.1488
		L3 = 0.1546
		L4 = 0.1546
		L5 = 0.1546
		L10 = 0.1546
		L15 = 0.1546
		L18 = 0.1546
		L19 = 0.1546
		L20 = 0.1541
		L30 = 0.1522
		L50 = 0.1344
		L100 = 0.1264
		LInf = 0.1077
	I(Draw,Class) = 0.4679
    I(Checkmate,Class) = 0.0110

Use L3 norm!

Entropy(Base Data):
	H(Class) = 3.5042


Decision Tree:
    Base data:
        Tree has 27429 nodes. Perfect accuracy.
    Base data + meta-features:
        Tree has 26751 nodes for L2. Perfect accuracy.
        Tree has 27047 nodes for L3. Perfect accuracy.
    Two trees above seem to MASSIVELY overfit training data. Also, my meta-
     features suck.
    Meta_features:
        Tree has 95 nodes. ~40% raw accuracy, 90% of values are within 3
        classes of actual value.