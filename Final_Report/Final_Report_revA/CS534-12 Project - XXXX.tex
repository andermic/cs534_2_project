% Template Taken from NIPS 2012
%
% Document Properties
\documentclass[fleqn]{article}
\usepackage{nips12submit_e,times}
%
% Inserted Packages (Gregory Added)
\usepackage{enumerate}
\usepackage{appendix}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm, algorithmic}
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
%
%-------------------------------------------------------------------------------
\title{Prediction of Chess Endgame using\\
Decision Tree and SVM Classifiers}

\author{
Anderson, Michael\\
CS\\ 
\texttt{andermic@eecs.oregonstate.edu} \\
\AND
Gutshall, Gregory\\
ECE\\
\texttt{gutshalg@eecs.oregonstate.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
Insert Abstract Text Here.
\end{abstract}

% ***********************************************
\section{Introduction}
\label{sec:Intro}
%
\subsection[Background]{Background\textbackslash Problem Formulation}
\label{subsec:Background}
Discuss what a Chess Endgame is.  Discuss how one would go about determining a Chess Endgame?  
%
\subsection{Outline of Report}
\label{subsec:outline}
In section(\ref{sec:Dataset}) we describe the dataset from the UCI repository used for our training and testing environment.  We also provide insight into the parameterizations of the original data and why we think those parameterizations will yield improved classification results.  In section(\ref{sec:Theory}) we discuss the theory and limitations of the two proposed classification methods.  In section(\ref{sec:Results}) we will show results heuristically drawn from simulations for the two proposed methods and discuss results related to these findings.  Finally, in section(\ref{sec:Conclusions}) we will make final conclusions and possible algorithmic strategies to improve the results. 

% ***********************************************
\section{Dataset}
\label{sec:Dataset}

\subsection{Chess (King-Rook vs. King) Data Set}
\label{subsec:dataset}
Discuss the dataset from UCI\cite{MichaelBain:1994}.  Format of the data.

\subsubsection{Notation}

\paragraph{Chess Board Positions:}
Let the following notation describe the space of a Chess board,
\begin{equation}
\label{eq:StandardPositions}
\begin{aligned}
	\text{File} &\in \left[a,b,c,d,e,f,g,h\right]\\
	&\in \left[1,2,3,4,5,6,7,8\right]\\
	\text{Rank} &\in \left[1,2,3,4,5,6,7,8\right]\\
\end{aligned}
\end{equation}

\paragraph{Game Pieces:}
Let the three game pieces be defined as $Piece_i$, where $i = \left[1,2,3\right]$ represents the piece index,
\begin{equation}
\label{eq:pieces}
\begin{aligned}
	Piece &\in \left[W_k, W_r, B_k\right]\\
	&W_k = \text{White King}\\
	&W_r = \text{White Rook}\\
	&B_k = \text{Black King}\\
\end{aligned}
\end{equation}

\paragraph{Examples:}
Let a example of a game be defined as $Game_j$ where $j = \left[1,2,3,\dots\right]$ is the game row index, 
%
\begin{equation}
\begin{aligned}
	Game_j &= \left[Piece_i \left\{file_j, rank_j\right\}\right]\\
\end{aligned}
\end{equation}
%
The entire set of examples is labeled as $\mathbf{X}$.

\paragraph{Class Labels:}
Class labels are defined as the remaining moves till checkmate of $B_k$.  Note, checkmate of $B_k$ is called on the $m^{\text{th}}$ move of $B_k$.
\begin{equation}
\begin{aligned}
	y_j &\in \left[draw,0,1,2,3,4, \dots, m\right]\\
	&\in \left[-1,0,1,2,3,4, \dots, m\right]
\end{aligned}
\end{equation}
%
The entire set of training class labels is labeled as $\mathbf{y}$.

\subsection{Parameterization}
\label{subsec:Parameterization}
Discuss how we parameterized the data.

Several parameter functions are used to classify or achieve an objective function, these parameter functions are labeled as $\mathbf{\Phi}\left(\mathbf{X}\right)$. 

% ***********************************************
\section{Theory of Proposed Methods}
\label{sec:Theory}

\subsection{Theory: Decision Trees}
\label{sub:theoryDecisionTrees}
Theory goes here.

\subsection{Theory: Support Vector Machines (SVM)}
\label{sub:theorySVM}

\subsubsection{Binary SVM Classification}
\label{sub:binarySVM}
A support vector machine (SVM) attempts to draw a decision boundary between two classes, which results in the maximum margin\cite{bishop:2006}.  Margin being defined as the orthogonal distance from the decision boundary to a subset of support points $\mathbf{x}_{support} \subset \mathbf{X}$.  Since, we have freedom of normalizing the decision space we set this boundary to $1$ and then attempt to solve the following quadratic optimization problem,
%  
\begin{equation}
\label{eq:SVM}
\begin{aligned}
	& \underset{w,b}{\text{min}}
	& & \frac{1}{2} \left\|\mathbf{w}\right\|^2 \\
	& \text{subject to}
	& & y_i\left(\mathbf{w}^T \mathbf{x}_i + b\right) \geq 1 \text{ , } i = 1,\dots,N
\end{aligned}
\end{equation}
%
The above equation is useful for the linearly separable Hard-Margin case.  Since the dataset(\ref{subsec:dataset}) contains overlapping data points for different classes, we require a non-linearly separable SVM, i.e. we need to incorporate a Soft-Margin.  This is achievable by relaxing the normalized constraint to allow support points to be within the margin\cite{UsersGuideSVM}.  The common approach, from Linear Programming, shows that adding slack variables $\boldsymbol{\xi}$ along with a trade-off parameter $C$ can achieve this,
%
\begin{equation}
\label{eq:SVM_soft}
\begin{aligned}
	& \underset{w,b}{\text{min}}
	& & \frac{1}{2} \left\|\mathbf{w}\right\|^2 + C \sum_{i=1}^N \xi_i\\
	& \text{subject to}
	& & y_i\left(\mathbf{w}^T \mathbf{x}_i + b\right) \geq 1 - \xi_i \text{ , } \xi_i \geq 0 \text{ , } i = 1,\dots,N 
\end{aligned}
\end{equation}
%
Now, it was observed from the dataset(\ref{subsec:dataset}) that the number of examples for each class was vastly different.  In this case, the SVM approximates a majority-class classifier and places the decision boundary extremely close to the minority class.
%
\begin{figure}[H]
	{\centering
		\includegraphics[trim = 2mm 2mm 2mm 2mm, clip,width=0.7\textwidth]{figs/DifferentC}
		\caption{Using different trade-off parameters $\mathbf{C}$ to prevent majority-control (figure from \cite{UsersGuideSVM})}
		\label{fig:Different_SVM_C_size}
	}
\end{figure}
%
To avoid majority-control, we can scale the trade-off parameters $\mathbf{C} \in [C_{1}, C_{-1}]$ so that the minority class caries more weight when misclassified. Such that eq(\ref{eq:SVM_soft}) becomes,
%
\begin{equation}
\label{eq:scaledC}
\begin{aligned}
	\mathbf{C} \sum_{i=1}^N \xi_i &= C_1 \sum_{i \in +}^N \xi_i + C_{-1} \sum_{i \in -}^N \xi_i\\
	& C_1 = \frac{n_{-1}}{n_{1}} C_{-1}
\end{aligned}
\end{equation}
%
So far eq(\ref{eq:SVM_soft}) uses a linear kernel for the inner product of $\mathbf{w}^T \mathbf{x}_i$.  We can expand this to a larger dimensional space by using a polynomial kernel $K(x_i , x_j) = (\gamma x_i^T x_j + r)^d$ of various power $d$, a radial basis function (Gaussian) kernel $K(x_i , x_j) = \exp(-\gamma \left\|x_i - x_j\right\|^2)$, or a sigmoid kernel $K(x_i , x_j) = \tanh(\gamma x_i^T x_j + r)$.  All three kernels were attempted using the LIBSVM solver\cite{LIBSVM} (see, sec(\ref{sub:resultsSVM})).

One method employed for this report was \textit{Pegasos: Primal Estimated sub\_GrAdient SOlver for SVM}\cite{Shwartz:2007}.  This is a iterative shrinkage method that alternates between stochastic gradient descent steps and projection steps.  This method does \textbf{not} improve accuracy over other SVM methods, but does drastically improve convergence time.  This speedup is achievable by approximating the sub-gradient with $k$ random samples of the training matrix and projecting $\mathbf{w}$ onto a $L_2$ ball of radius $1/\sqrt{\lambda}$, where $\lambda$ and $k$ are tunning parameters.      

\subsubsection{Multi-Class SVM Classification}
\label{sub:multiSVM}
LIBSVM comes with its own methods for handling multi-class classification.  To the end-user it is more of a \textit{``black-box''}, where $\mathbf{X}, \mathbf{y}, \text{and some [options]}$ are supplied to the function verbatim.  

For Pegasos, we tried two methods for multi-class classification.  The first, was \textit{one-versus-the-rest}\cite{vapnik:1998}, where $K$ separate SVMs are trained, with the $k^{th}$ class assigned $+1$ and all other classes assigned to $-1$.  Then the classification is provided by $y(\mathbf{x}) = \text{max}_k y_k (\mathbf{x})$.  This provided dismal results, since the decision boundary tended to split the feature space into separate halves, giving classes $\widehat{y} = 1$ and $\widehat{y} = 16$ the farthest distance to any decision boundary. The second method, was \textit{one-versus-one}\cite{vapnik:1998}, where $K(K-1)$ separate SVMs are trained against each other.  Then the classification is provided by $y(\mathbf{x}) = \text{max}_k \rho_k y_k (\mathbf{x})$.  Where, $\rho_k$ is the number of ``votes'' for a given class $y_k$.

% ***********************************************
\section[Results]{Simulation\textbackslash Classification Results}
\label{sec:Results}

\subsection{Results: Decision Tree}
\label{sub:resultsTrees}

\subsection{Results: Support Vector Machine (SVM)}
\label{sub:resultsSVM}

\subsubsection{Pegasos}
\label{subsub:Pegasos}
To start, the Pegasos algorithm\cite{Shwartz:2007} was visually tested against the chosen parameter space to see how separable the data was (see, fig(\ref{fig:2D_Space})) and if the algorithm was working correctly.  
%
\begin{figure}[H]
	{\centering
		\includegraphics[trim = 10mm 8mm 10mm 0mm, clip,width=1.0\textwidth]{figs/2D_FeatureSpace}
		\caption{Feature Space Separation for two-parameters ($\left\|B_k - W_k\right\|^2$ and $\left\|B_k - corner\right\|^2$) for classes $y=4$ and $y=16$}
		\label{fig:2D_Space}
	}
\end{figure}
%
With the algorithm working, the next goal is to setup the grid-search for tunning parameters $\lambda$ and $k$.  The projection distance is controlled by $\lambda$, while the subset of training examples for the sub-gradient is controlled by $k$, i.e. $A_k \subseteq X$.  This grid-search had to be performed on each inner-class binary training problem. 
%
\begin{figure}[H]
	{\centering
		\includegraphics[trim = 10mm 8mm 10mm 0mm, clip,width=1.0\textwidth]{figs/Grid_Search}
		\caption{Grid-search over $\lambda$ and $k$ for a single binary classification instance}
		\label{fig:GridSearch}
	}
\end{figure}  
%
For the multi-class problem, we used the one-vs-one method with a voting system defined by $y(\mathbf{x}) = \text{max}_k \rho_k y_k (\mathbf{x})$.  The voting parameter $\rho_k = \sum_{i \neq I_k} (y_i = +1)$ was created by evaluating each row of the weight matrix, corresponding to a class label $\mathbf{y} \in [1,2,\dots,16]$, such that $y_i = sign(\mathbf{W}_i^T \mathbf{x} + \mathbf{b}_i)$.  The final multi-class prediction was found to be,

\begin{table}[htbp]
	\centering
		\begin{tabular}{ | l | c | c | c |}
  		\hline                        
  		\textbf{Method} & \textbf{$\mu$ error} & \textbf{$\sigma$ error} & \textbf{Testing Accuracy} \\ \hline
  		Pegasos: One-v-One & 1.78 & 1.59 & 51.25\% \\ \hline
		\end{tabular}
	\label{table:AccuracyPegasos}
\end{table}
Here, testing accuracy is taken to be within a $\epsilon$-ball away from the true class label.  For this report, we used $\epsilon = 1$.  Note, since this is a multi-class problem the testing accuracy expectation for random guessing across 16 class labels is $P_y = 1/16 = 6.25\%$.

\subsubsection{LIBSVM}
\label{subsub:LIBSVM}
As mentioned, LIBSVM\cite{LIBSVM} is a off-the-shelf implementation for SVM.  We tried several different kernels with our dataset (again, using $\epsilon = 1$ for the testing accuracy) and found the following results,

\begin{table}[htbp]
	\centering
		\begin{tabular}{ | l | c | c | c |}
  		\hline                        
  		\textbf{LIBSVM: Kernel} & \textbf{$\mu$ error} & \textbf{$\sigma$ error} & \textbf{Testing Accuracy} \\ \hline
  		 Linear & 1.29 & 1.42 & 68.28\% \\ \hline
  		 Polynomial d-2 & 1.75 & 1.93 & 59.9\% \\ \hline
  		 Polynomial d-3 & 1.32 & 1.42 & 66.7\% \\ \hline
  		 Polynomial d-4 & 1.60 & 1.71 & 61.82\% \\ \hline
  		 RBF & 1.20 & 1.37 & 70.1\% \\ \hline
  		 Sigmoid & 1.94 & 1.85 & 52.5\% \\ \hline
		\end{tabular}
	\label{table:AccuracyLIBSVM}
\end{table}

% ***********************************************
\section{Conclusions}
\label{sec:Conclusions}

% ***********************************************
%\nocite{*}  %Uncomment this line to print all citations 
\bibliographystyle{IEEEbib}
\bibliography{references} 

\end{document}